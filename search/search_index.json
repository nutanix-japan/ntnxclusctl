{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p><code>Always be Scaling to Zero!</code></p> <p>Now you may be sick of hearing this one too many times. Perhaps you want to see this in action and see what the fuss is all about? Perhaps you are person that wants to get your hands dirty?</p> <p>Perhaps the following japanese sentence makes much sense to you?</p> <p><code>\u624b\u3092\u52d5\u304b\u3057\u3066\u898b\u306a\u3044\u3068\u3002\u3002</code></p> <p>Then read on..</p> <p>Welcome to the EXPERIMENTAL Serverless labs. This is not intended for PRODUCTION purposes.</p> <p>Most of the project works around Serverless capabilities that can be provided on Nutanix. (EXPERIMENTAL)</p> <p>All the experiments are inspired by content in Nutanix Opendocs documents.</p> <p>This repository/Gitpages is just a way for us to gather and document all that we have tested and worked with and share with like-minded people.</p>"},{"location":"#our-perceived-evolution-of-serverless","title":"Our Perceived Evolution of Serverless","text":""},{"location":"#then-cloud-provider-controlled","title":"Then - Cloud Provider Controlled","text":"<p>You would logon to a cloud provider console and input your function and cloud provider took care of running the funtion. The OS, the infrastructure required to run your code. </p> <p>At the end of the billing period you got a bill for the amount of CPU and memory you used for a period of time.</p>"},{"location":"#now-customer-controlled","title":"Now - Customer Controlled","text":"<p>Customers are increasingly required to do more with less. Emergence of Kubernetes is allowing customers to gain control over cloud spending by automating resource consumption only when they are required. There is no requirement to have infrastructure running if no workloads are running. Automating launching required resources on-demand, scaling up and scaling them down makes sense for customers.</p> <p>This gives customer tight integration of infrastructure resources with their applications.</p>"},{"location":"#lab-flow","title":"Lab Flow","text":"<p>This lab will introduce you the afore mentioned customer controlled serverless scenario. </p> <p>All these steps are done on Nutanix hybrid cloud platform which can be installed on-premises and on a cloud provider of your choice (AWS and Azure for now).</p> <p>The flow works in the following manner:</p> <pre><code>stateDiagram\n    direction LR\n    BuildVMImage --&gt; CreateManagementCluster\n    state BuildVMImage{\n        direction LR\n        Sucessful? --&gt; UploadtoPC\n    }\n    CreateManagementCluster --&gt; CreateWorkloadCluster\n    state CreateManagementCluster {\n      direction LR\n      UseKind? --&gt; WaitForUp\n    }\n    CreateWorkloadCluster --&gt; TestScalingEvents\n    state CreateWorkloadCluster {\n      direction LR\n      Success? --&gt; GetKubeConfig\n    }\n    state TestScalingEvents {\n      direction LR\n      ScaleUp --&gt; ScaleDown\n    }</code></pre> <ol> <li>Using Hashicorp Packer tool you will create VM image for Kubernetes OS - this image will be uploaded to Prism Central </li> <li>You will create a Management Kubernetes Cluster on your Laptop (Kind suggested here) - please feel free to use your installation tool and platform</li> <li>You will deploy a Workload Kubernetes Cluster from your Management Kubernetes Cluster</li> <li>You will configure Autoscaler capability on Management Kubernetes Cluster</li> <li>You will deploy a workload on the Workload Kubernetes cluster</li> <li>You will test Scaling events on the workload in the Workload Kubernetes cluster</li> </ol> <p>Happy Scaling to Zero!</p>"},{"location":"autoscaler_testing/","title":"Testing Autoscaling Functionality","text":"<p>Warning<p>As of April 2023, Autoscaling is a <code>yet to be validated</code> feature and it is <code>not intended</code> for production purposes.</p> <p>For updates follow this Nutanix Opendocs document.</p> </p>"},{"location":"autoscaler_testing/#section-flow","title":"Section Flow","text":"<pre><code>stateDiagram\n    direction LR\n    state TestScalingEvents {\n      direction LR\n      ScaleUp --&gt; ScaleDown\n    }</code></pre> <p>We will run with the following configuration:</p> <ol> <li>The Management cluster runs the Autoscaler </li> <li>The worker nodes runs the workloads</li> <li>The Autoscaler on the management cluster monitors the resource requirements of the workloads in the workload cluster and scales up and scales in the worker nodes in the workload cluster</li> </ol> <pre><code>graph LR\n  B[Management Cluster - AutoScaler ] --&gt;|\"[kubeconfig]\"| C[Workload Cluster - Workloads ];</code></pre>"},{"location":"autoscaler_testing/#deploy-autoscaler-on-management-cluster","title":"Deploy AutoScaler on Management Cluster","text":"<ol> <li> <p>Make sure your Kubernetes context is that of the your Management cluster.</p> <p><pre><code>export KUBECONFIG=${MGMT_KUBECONFIG_FILE}\n</code></pre> Run additional commands to make sure you are the Management cluster<pre><code>kubectl get nodes\nkubectl get ns\n</code></pre></p> </li> <li> <p>Download the AutoScaler manifest</p> <pre><code>curl -OL https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/clusterapi/examples/deployment.yaml\n</code></pre> </li> <li> <p>Replace the environment variable in the dowloaded one to suit our workload cluster namespace environemnt variabele ${WORKLOAD_CLUSTER_NS}</p> <pre><code>sed -i 's/${AUTOSCALER_NS}/${WORKLOAD_CLUSTER_NS}/g' deployment.yaml\n</code></pre> </li> <li> <p>Set your environment variables for the Autoscaler image you would like to use </p> <pre><code>export AUTOSCALER_IMAGE=\"us.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler:v1.24.1\"\n</code></pre> <p>Info</p> <p>Other versions of <code>Autoscaler</code> images can be found here.</p> </li> <li> <p>Modify and add (where necessary) the following highlighted sections of the <code>deployment.yaml</code> file (specifically inside the container section)</p> <pre><code>containers:\n- image: ${AUTOSCALER_IMAGE}\nname: cluster-autoscaler\ncommand:\n- /cluster-autoscaler\nargs:\n- --cloud-provider=clusterapi\n- --kubeconfig=/mnt/kubeconfig/kubeconfig.yml\n- --clusterapi-cloud-config-authoritative\n- -v=1\nvolumeMounts:\n- mountPath: /mnt/kubeconfig\nname: kubeconfig\nreadOnly: true\nvolumes:\n- name: kubeconfig\nsecret:\nsecretName: ${WORKLOAD_CLUSTER_NAME}-kubeconfig # (1)\nitems:\n- key: value\npath: kubeconfig.yml\n</code></pre> <ol> <li> Make sure to use the name/env. variable of your workload cluster. Another easy way to find the secret name, to be used, is to run the command <code>kubectl get secrets -n $AUTOSCALER_NS</code>.</li> </ol> </li> <li> <p>Check the manifest to make sure you have included all details, and apply the <code>Autoscaler</code> deployment manifest</p> <pre><code>kubectl apply -f deployment.yaml </code></pre> <p>Make sure the autoscaler pod is running </p> <pre><code>k get po -n ${WORKLOAD_CLUSTER_NS}  #                                                                            \nNAME                                  READY   STATUS    RESTARTS   AGE\ncluster-autoscaler-6dbb469585-4ggtd   1/1     Running   0          7s\n</code></pre> <p>Warning<p>Do not proceed if the <code>Autoscaler</code> pod has issues starting, troubleshoot and fix before moving on to the next section.</p> </p> </li> </ol> <p>We have prepared the Autoscaler resource to manage resources in our workload cluster.</p>"},{"location":"autoscaler_testing/#capacity-management","title":"Capacity Management","text":"<p>As a responsible Solution Architect and an Administrator you need to do plan for resources and manage capacity as well. </p> <p>For this reason let us assume that you have decided that you will provide the following resources to the application team.</p> VMs High Limit 5 Low Limit 1 <p>To apply this capacity in the Autoscaler we need to set annotations in <code>MachineDeployment</code> resource.</p> <p>Edit the <code>MachineDeployment</code> resource using the following command </p> <p><pre><code>kubectl edit MachineDeployment ${WORKLOAD_CLUSTER_NAME}-wmd -n ${WORKLOAD_CLUSTER_NS}\n</code></pre> Under the <code>metadata</code> section paste the following two lines:</p> <pre><code>cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \"5\"\ncluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \"1\"\n</code></pre> <p>Your MachineDeployment metadata section would look something like this  <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: MachineDeployment\nmetadata:\nannotations:\ncluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \"5\" # (1)\ncluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \"1\" # (2)\n</code></pre></p> <ol> <li> Specifies maximum number of worker nodes to scale up to </li> <li> Specifies minimum number of worker nodes </li> </ol>"},{"location":"autoscaler_testing/#testing-scaling-events","title":"Testing Scaling Events","text":"<p>Now comes the fun part that we have been setting up for. </p> <p>Let us deploy a test workload on our workload cluster and check if scaling events actually work.</p> <ol> <li> <p>Apply the following workload manifest</p> <pre><code>k --kubeconfig ${WORKLOAD_CLUSTER_NAME}.cfg apply -f https://k8s.io/examples/application/php-apache.yaml\n</code></pre> <p>This will start just one pod.</p> </li> <li> <p>Scale up this Deployment to 100 pods which will require more than one worker node worth of resources</p> <pre><code>k --kubeconfig ${WORKLOAD_CLUSTER_NAME}.cfg scale deployment php-apache --replicas 100\n</code></pre> </li> <li> <p>Watch the AutoScaler <code>PHASE</code> column in the output</p> <p>Sample output<pre><code>kubectl get MachineDeployment -A        </code></pre> <pre><code>NAMESPACE                 NAME           CLUSTER    REPLICAS   READY   UPDATED   UNAVAILABLE   PHASE       AGE   VERSION\n${WORKLOAD_CLUSTER_NS}    kubevip3-wmd   kubevip3   5          4       5         1             ScalingUp   22h   v1.24.11\n</code></pre></p> </li> <li> <p>Watch the AutoScaler logs by running the following command</p> Command TemplateCommand Sample <pre><code>k logs &lt;name of your AutoScaler pod&gt; -n ${WORKLOAD_CLUSTER_NS}  -f # (1)\n</code></pre> <pre><code>k logs cluster-autoscaler-6dbb469585-4ggtd -n ${WORKLOAD_CLUSTER_NS}  -f \n#\n#\nI0413 09:32:53.208911       1 scale_up.go:472] Estimated 5 nodes needed in MachineDeployment/kubevip3ns /kubevip3-wmd\nI0413 09:32:53.406155       1 scale_up.go:595] Final scale-up plan: [{MachineDeployment/kubevip3ns /kubevip3-wmd 1-&gt;5 (max: 5)}]\nI0413 09:32:53.406215       1 scale_up.go:691] Scale-up: setting group MachineDeployment/$kubevip3ns /kubevip3-wmd size to 5\nW0413 09:33:04.902674       1 clusterapi_controller.go:469] Machine \"kubevip3-wmd-57fcdf9f7xbgz8z-kcx9h\" has no providerID\nW0413 09:33:04.902700       1 clusterapi_controller.go:469] Machine \"kubevip3-wmd-57fcdf9f7xbgz8z-m88fl\" has no providerID\nW0413 09:33:04.902708       1 clusterapi_controller.go:469] Machine \"kubevip3-wmd-57fcdf9f7xbgz8z-mlwb6\" has no providerID\nW0413 09:33:04.902713       1 clusterapi_controller.go:469] Machine \"kubevip3-wmd-57fcdf9f7xbgz8z-rthbj\" has no providerID                                                                                      \n</code></pre> </li> <li> <p>You can also watch all the 100 pods now running using the following command</p> <p><pre><code>k --kubeconfig ${WORKLOAD_CLUSTER_NAME}.cfg get pods       </code></pre> <pre><code>NAME                          READY   STATUS             RESTARTS   AGE\nphp-apache-698db99f59-24mqp   1/1     Running            0          17m\nphp-apache-698db99f59-2pcpz   1/1     Running            0          17m\nphp-apache-698db99f59-2vt28   1/1     Running            0          17m\nphp-apache-698db99f59-2x8zs   1/1     Running            0          17m\nphp-apache-698db99f59-44tf4   1/1     Running            0          17m\nphp-apache-698db99f59-45vcp   1/1     Running            0          17m\nphp-apache-698db99f59-4f9j6   1/1     Running            0          17m\n</code></pre></p> </li> <li> <p>Let us check the number of nodes in the workload cluster and see that it has been scaled up to 5 </p> <pre><code>k --kubeconfig ${WORKLOAD_CLUSTER_NAME}.cfg get nodes                                                                 NAME                                 STATUS   ROLES           AGE     VERSION\nkubevip3-kcp-jnhf5                   Ready    control-plane   22h     v1.24.11\nkubevip3-kcp-p56j9                   Ready    control-plane   22h     v1.24.11\nkubevip3-kcp-z8pqj                   Ready    control-plane   22h     v1.24.11\nkubevip3-wmd-57fcdf9f7xbgz8z-9mf59   Ready    &lt;none&gt;          7m51s   v1.24.11\nkubevip3-wmd-57fcdf9f7xbgz8z-kcx9h   Ready    &lt;none&gt;          19m     v1.24.11\nkubevip3-wmd-57fcdf9f7xbgz8z-m88fl   Ready    &lt;none&gt;          19m     v1.24.11\nkubevip3-wmd-57fcdf9f7xbgz8z-mlwb6   Ready    &lt;none&gt;          19m     v1.24.11\nkubevip3-wmd-57fcdf9f7xbgz8z-pqp8s   Ready    &lt;none&gt;          22h     v1.24.11\n</code></pre> </li> <li> <p>Now we can test a scale down event to see if AutoScaler is communicating with Prism Central APIs to delete VMs that are not necessary</p> <pre><code>k --kubeconfig ${WORKLOAD_CLUSTER_NAME}.cfg scale deployment php-apache --replicas 10\n</code></pre> </li> <li> <p>Watch the Node count, pods count, and Deployment logs as before.</p> </li> </ol> <p>You have experienced one of your many serverless compute experience. </p> <p>Do come back to this site check for more updates.</p>"},{"location":"build_image/","title":"Build Kubernetes VM Image","text":"<p>In this section we will build a VM image using Hashicorp Packer Automation.</p> <pre><code>stateDiagram\n    direction LR\n    BuildVMImage --&gt; CreateManagementCluster\n    state BuildVMImage{\n        direction LR\n        Sucessful? --&gt; UploadtoPC\n    }</code></pre> <p>This image will be uploaded to Prism Central which will then be used by <code>clusterctl</code> utility to build VMs upon which Kubernetes will be deployed.</p> <p>Since the image needs quite a bit of packages to be installed it makes sense for this process to be automated. It is a good idea to have this image immutable so production workloads are not disturbed by OS or software upgrades. </p> <p>Packages include the following (not limited to):</p> <ul> <li>Kubernetes binaries (version determined by logic in this <code>image-builder/images/capi/hack/image-new-kube.py</code> script -  you will download as you follow steps in this lab)</li> <li>CSI drivers</li> <li>Containerd binaries</li> </ul>"},{"location":"build_image/#step-to-build-a-workstation","title":"Step to Build a Workstation","text":"<p>Follow the steps here to build a workstation </p>"},{"location":"build_image/#steps-to-building-image","title":"Steps to Building Image","text":"<p>Note</p> <p>All steps to build the VM image are from Nutanix section of the Open Source Image Builder Book</p> <ol> <li> <p>Login to your workstation </p> </li> <li> <p>Ensure you have git installed</p> <pre><code>git version\n</code></pre> </li> <li> <p>Clone this repository</p> <pre><code>git clone https://github.com/kubernetes-sigs/image-builder.git\n</code></pre> </li> <li> <p>Run the command to install Ansible and Packer</p> <pre><code>cd image-builder/images/capi/\nmake deps-nutanix\n</code></pre> </li> <li> <p>Change to the following directory and provide your Nutanix Prism Central information. This is where you will build and store the image</p> <pre><code>cd packer/nutanix/\nvi nutanix.json\n</code></pre> Template fileSample file <pre><code>{\n\"force_deregister\": \"true\",\n\"nutanix_cluster_name\": \"Name of PE Cluster\",\n\"nutanix_endpoint\": \"Prism Central IP / FQDN\",\n\"nutanix_insecure\": \"true\",\n\"nutanix_password\": \"PrismCentral_Password\",\n\"nutanix_port\": \"9440\",\n\"nutanix_subnet_name\": \"Name of Subnet\",\n\"nutanix_username\": \"PrismCentral_Username\",\n\"scp_extra_vars\": \"\"\n}\n</code></pre> <pre><code>{\n\"force_deregister\": \"true\",\n\"nutanix_cluster_name\": \"pe.example.com\",\n\"nutanix_endpoint\": \"pc.example.com\",\n\"nutanix_insecure\": \"true\",\n\"nutanix_password\": \"XXXXXX\",\n\"nutanix_port\": \"9440\",\n\"nutanix_subnet_name\": \"MY-PE-SUBNET-VLAN-0\",\n\"nutanix_username\": \"admin\",\n\"scp_extra_vars\": \"\"\n}\n</code></pre> </li> <li> <p>Once this file is saved with your Nutanix cluster specific information. Run the following command to intialise and build the <code>ubuntu-2204</code> image.</p> <pre><code>make build-nutanix-ubuntu-2204\n</code></pre> <p>Tip</p> <p>You can also logon to Prism Central UI to check the Tasks to see the Image build activities.</p> <p>Feel free to build other OS images as well and use them as OS in your kubernetes clusters. </p> Command output - This will take about 13 minutes<pre><code>&lt; output snipped\u3000&gt;\n==&gt; nutanix: Creating image(s) from virtual machine ubuntu-2204-kube-v1.24.11...\n nutanix: Found disk to copy: SCSI:0\n nutanix: Image successfully created: ubuntu-2204-kube-v1.24.11 (ec76c3ce-57c9-4f4f-b9fb-1e3f92157280)\n==&gt; nutanix: Deleting virtual machine...\n    nutanix: Virtual machine successfully deleted\n==&gt; nutanix: Running post-processor: custom-post-processor (type shell-local)\n==&gt; nutanix (shell-local): Running local shell script: /var/folders/fn/40xgq_sx74s6qbr42534mbwc0000gn/T/packer-shell2304847137\nBuild 'nutanix' finished after 13 minutes 23 seconds.\n\n==&gt; Wait completed after 13 minutes 23 seconds\n==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; nutanix: ubuntu-2204-kube-v1.24.11\n--&gt; nutanix: ubuntu-2204-kube-v1.24.11  # &lt;&lt;&lt; note this down\n</code></pre> </li> </ol> <p>You can use the <code>ubuntu-2204-kube-v1.24.11</code> image name in the next section to create VMs on which you will install Kubernetes.</p>"},{"location":"create_management_cluster/","title":"Creating a Management Cluster","text":""},{"location":"create_management_cluster/#purpose-of-management-cluster","title":"Purpose of Management Cluster","text":""},{"location":"create_management_cluster/#workstation-setup","title":"Workstation Setup","text":"<pre><code>stateDiagram\n    direction LR\n    CreateManagementCluster --&gt; CreateWorkloadCluster\n    state CreateManagementCluster {\n      direction LR\n      UseKind? --&gt; WaitForUp\n    }</code></pre> <p>We will need a Kubernetes cluster setup on our workstation. This could be any of the following:</p> <ul> <li>Kind</li> <li>Minikube</li> <li>k3d</li> </ul> <p>During testing on a Intel Macbook pro with 8 CPU and 16 CPU, we found out that <code>kind</code> cluster performed the best.</p> <p>Please feel free to choose what you are familiar with and have proven sucess with.</p> <p>Warning<p>We have noticed the following issues when we were running kubernetes clusters on local Mac/Windows</p> <ul> <li>Pods on <code>kube-system</code> namespace may take about 10 minutes to download</li> <li>Pods may take about 5 minutes to become <code>Ready</code> state</li> <li>Make sure the management cluster is fully functional before proceeding to creating workload cluster</li> </ul> </p>"},{"location":"create_management_cluster/#create-management-cluster","title":"Create Management Cluster","text":"<p>Follow steps here to create a <code>kind</code> mangement cluster </p>"},{"location":"create_management_cluster/#install-clusterctl","title":"Install clusterctl","text":"<p>Follow procedure here to install <code>clusterctl</code> tool </p> <p>A typical linux install command would like this, but do refer the link above for the latest command and your installation platform. </p> Installation for AMD64 Platform<pre><code>curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.4.1/clusterctl-linux-amd64 -o clusterctl\n</code></pre>"},{"location":"create_management_cluster/#initialise-management-cluster","title":"Initialise Management Cluster","text":"<ol> <li> <p>Create the following file in your workstation </p> <pre><code>vi ~/.cluster-api/clusterctl.yaml\n</code></pre> </li> <li> <p>Fill in the following contents</p> <pre><code>NUTANIX_ENDPOINT: \"\"     # IP or FQDN of Prism Central\nNUTANIX_USER: \"admin\"             # Prism Central user\nNUTANIX_PASSWORD: \"\"    # Prism Central password\nNUTANIX_INSECURE: true\nEXP_CLUSTER_RESOURCE_SET: true    # Experimental for testing CCM and Autoscaling\nEXP_MACHINE_POOL: \"true\"          # Experimental\nCLUSTER_TOPOLOGY: \"true\"          # Experimental\nKUBERNETES_VERSION: \"v1.24.7\"     # use the kubernetes version from the image build information\nWORKER_MACHINE_COUNT: 3\nNUTANIX_MACHINE_VCPU_SOCKET: 4\nNUTANIX_MACHINE_MEMORY_SIZE: \"8Gi\"\nNUTANIX_SSH_AUTHORIZED_KEY: \"ssh-rsa AAAAB3....\"\nNUTANIX_PRISM_ELEMENT_CLUSTER_NAME: \"\"\nNUTANIX_MACHINE_TEMPLATE_IMAGE_NAME: \"xxxxxx\"    # use your image name from the previous section\nNUTANIX_SUBNET_NAME: \"Primary\"\n</code></pre> </li> <li> <p>Instantiate the Cluster API to communicate with Nutanix Cluster</p> <p><pre><code>clusterctl init -i nutanix\n</code></pre> 4. Make sure all pods are in a Running and healthy state in the following namespaces:</p> <pre><code>kubectl get pods -n capi-system\nkubectl get pods -n capx-system\nkubectl get pods -n cert-manager\n</code></pre> <p>Warning</p> <p>If pods in the above name spaces are not running or have any errors, troubleshoot your CAPX deployment on management cluster. </p> </li> </ol>"},{"location":"create_workload_cluster/","title":"Creating a Workload Cluster","text":"<p>In this section we will create a Workload cluster. </p> <p>This workload cluster will be deployed from the Mangement cluster that we created in the previous section.</p> <pre><code>stateDiagram\n    direction LR\n    CreateWorkloadCluster --&gt; TestScalingEvents\n    state CreateWorkloadCluster {\n      direction LR\n      Success? --&gt; GetKubeConfig\n    }</code></pre> <ol> <li> <p>Make sure your Kubernetes context is that of the your Management cluster.</p> <p><pre><code>export KUBECONFIG=${MGMT_KUBECONFIG_FILE}\n</code></pre> Run additional commands to make sure you are the Management cluster<pre><code>kubectl get nodes\nkubectl get ns\n</code></pre></p> </li> <li> <p>Reserve a new Control Plane endpoint IP for your Kubernetes cluster.</p> <pre><code>export WORKLOAD_CLUSTER_NAME=mycluster\nexport WORKLOAD_CLUSTER_NS=mynamespace\n</code></pre> </li> <li> <p>Generate your kubernetes clusters configuration file</p> <pre><code>CONTROL_PLANE_ENDPOINT_IP=x.x.x.x clusterctl generate cluster ${WORKLOAD_CLUSTER_NAME} --flavor ccm \\\n-i nutanix \\\n--target-namespace ${WORKLOAD_CLUSTER_NS}  \\\n--kubernetes-version v1.24.11 \\\n--control-plane-machine-count 1 \\\n--worker-machine-count 3 &gt; ./cluster.yaml\n</code></pre> </li> <li> <p>Create a namespace in the management cluster where the workload cluster can be managed</p> <pre><code>kubectl create ns ${WORKLOAD_CLUSTER_NS}\n</code></pre> </li> <li> <p>Apply the <code>cluster.yaml</code> manifest to create your workload cluster in the Nutanix infrastructure</p> <pre><code>k apply -f cluster.yaml                                                                  </code></pre> <pre><code># Output\nconfigmap/user-ca-bundle created\nconfigmap/nutanix-ccm created\nsecret/mycluster created\nsecret/nutanix-ccm-secret created\nclusterresourceset.addons.cluster.x-k8s.io/nutanix-ccm-crs created\nkubeadmconfigtemplate.bootstrap.cluster.x-k8s.io/mycluster-kcfg-0 created\ncluster.cluster.x-k8s.io/mycluster created\nmachinedeployment.cluster.x-k8s.io/mycluster-wmd created\nmachinehealthcheck.cluster.x-k8s.io/mycluster-mhc created\nkubeadmcontrolplane.controlplane.cluster.x-k8s.io/mycluster-kcp created\nnutanixcluster.infrastructure.cluster.x-k8s.io/mycluster created\nnutanixmachinetemplate.infrastructure.cluster.x-k8s.io/mycluster-mt-0 created\n</code></pre> </li> <li> <p>You can observe the creation of the VMs in the Prism Central/Element</p> </li> <li> <p>Download the kubeconfig for workload cluster by running the following command</p> <pre><code>clusterctl get kubeconfig ${WORKLOAD_CLUSTER_NAME} -n ${WORKLOAD_CLUSTER_NS} &gt; ${WORKLOAD_CLUSTER_NAME}.kubeconfig\nkubectl --kubeconfig ./${WORKLOAD_CLUSTER_NAME}.kubeconfig get nodes\n</code></pre> </li> <li> <p>Watch the nodes in the cluster until all the nodes come up (1 control plane and 3 workers unless you modified the <code>clusterctl.yaml</code> file)</p> <pre><code>kubectl --kubeconfig ./${WORKLOAD_CLUSTER_NAME}.kubeconfig get nodes -w\n</code></pre> </li> <li> <p>Once the desired number of nodes are present, you will see that these will be in a <code>NotReady</code> state. </p> <pre><code>k --kubeconfig mycluster.cfg get nodes                                              NAME                                 STATUS     ROLES           AGE     VERSION\nmycluster-kcp-jnhf5                   NotReady   control-plane   4m43s   v1.24.11\nmycluster-kcp-p56j9                   NotReady   control-plane   6m19s   v1.24.11\nmycluster-kcp-z8pqj                   NotReady   control-plane   2m13s   v1.24.11\nmycluster-wmd-57fcdf9f7xbgz8z-nkf45   NotReady   &lt;none&gt;          5m27s   v1.24.11\nmycluster-wmd-57fcdf9f7xbgz8z-pq47m   NotReady   &lt;none&gt;          5m21s   v1.24.11\nmycluster-wmd-57fcdf9f7xbgz8z-pqp8s   NotReady   &lt;none&gt;          5m23s   v1.24.11\n</code></pre> <p>For the nodes to become ready you need to install CNI. You must deploy a Container Network Interface (CNI) based pod network add-on so that your pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed.</p> </li> <li> <p>Run the following command to install Calico CNI.</p> <pre><code>kubectl --kubeconfig ./${WORKLOAD_CLUSTER_NAME}.kubeconfig apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml\n</code></pre> </li> <li> <p>Upon successful install of CNI, the nodes will get to a <code>READY</code> state. (it might take a minute or two depending on the resources available to your VMs)</p> </li> <li> <p>Confirm the state of your workload Kubernetes cluster</p> <p><pre><code>kubectl --kubeconfig ./${WORKLOAD_CLUSTER_NAME}.kubeconfig get nodes\n</code></pre> Output<pre><code>NAME                                 STATUS   ROLES           AGE     VERSION\nmycluster-kcp-jnhf5                   Ready    control-plane   8m46s   v1.24.11\nmycluster-wmd-57fcdf9f7xbgz8z-gsw4z   Ready    &lt;none&gt;          116s    v1.24.11\nmycluster-wmd-57fcdf9f7xbgz8z-pq47m   Ready    &lt;none&gt;          9m24s   v1.24.11\nmycluster-wmd-57fcdf9f7xbgz8z-pqp8s   Ready    &lt;none&gt;          9m26s   v1.24.11\n</code></pre></p> </li> <li> <p>If you have any issues or wanting to delete any workload cluster you can use the following commands</p> <p>To delete a workload cluster</p> <pre><code>kubectl delete cluster ${WORKLOAD_CLUSTER_NAME} -n ${WORKLOAD_CLUSTER_NS}\n</code></pre> </li> </ol>"},{"location":"appendix/create_kube/","title":"Creating a Kubernetes Cluster","text":""},{"location":"appendix/create_kube/#overview","title":"Overview","text":"<p>Before we can deploy an application using kubernetes we need to create a Kubernetes cluster first. The cluster that we are ging to create consists out of the following VMs:</p> <ul> <li>1 Master node (VM)</li> <li>1 Worker node (VM)</li> <li>1 etcd nodes (VM)</li> </ul> <p>Info<p>For more information on the terms master, worker and etcd, please look at https://kubernetes.io/docs/concepts/.</p> </p>"},{"location":"appendix/create_kube/#creating-a-karbon-cluster","title":"Creating a Karbon Cluster","text":"<ol> <li> <p>In your Prism Central, select the three dash in the top left corner and select Services &gt; Karbon</p> <p></p> </li> <li> <p>A new browser window will open and accept the HTTPS error you will see.</p> <p></p> </li> <li> <p>In the shown browser screen click on the + Create Cluster button for starting to create the kubernetes cluster</p> <p></p> </li> <li> <p>Provide the required parameters that are asked for in the wizard. The following screenshots can be used as a guideline</p> <p>Tip</p> <p>You can also hover over the ? sign to get more information about each configuration option.</p> </li> <li> <p>Choose Development Cluster</p> <p></p> </li> <li> <p>Enter a name for your cluster initials-cluster and choose your HPOC as Nutanix Cluster</p> <p></p> </li> <li> <p>Provide the Worker, Master and etcd settings as default; click Next</p> <p></p> </li> <li> <p>Provide the Network Provider settings as default; click Next</p> <p>Info<p>We use flannel as the network provider. More information on Flannel can be found here</p> </p> <p></p> </li> <li> <p>Provide the Storage class settings. For the cluster settings use admin and the cluster password that you used to login to the cluster.</p> <p></p> </li> <li> <p>Click on the Create button to have the cluster created by the system. Follow the process in the Karbon UI.</p> <p>Note<p>Based on the resources available on your cluster, it will take time. Wait until the cluster has been created before proceeding to the next part of the module!!</p> </p> <p></p> </li> </ol> <p>During the creation of the Kubernetes cluster there will have been created:</p> <ul> <li> <p>VMs</p> <p></p> </li> <li> <p>Persistent Storage as VolumeGroup</p> <p></p> <p></p> </li> </ul>"},{"location":"appendix/create_kube/#cluster-properties","title":"Cluster properties","text":"<p>In the Karbon UI, hover over the just created cluster (initials-cluster in our example) and click on it.</p> <p></p> <p>This will open another screen which shows the parts out of which the cluster is created according to the provided parameters that have been provided during the creation phase.</p> <p></p> <p>The below screenshots provide an example for the three parts, Storage Class, Volume and Add-on.</p> <p></p> <p></p> <p></p> <p>This concludes the end of this part of the module. You now have a running Kubernetes Cluster.</p>"},{"location":"appendix/create_kube/#takeaways","title":"Takeaways","text":"<ul> <li>It is quite easy to create a kubernetes cluster using Nutanix Karbon</li> <li>Both Development and Production cluster with different node count     options are available</li> <li>Flannel is the only CNI option available in the Karbon GUI. However,     Calico is avaialble to configure through REST API calls</li> </ul>"},{"location":"appendix/glossary/","title":"Glossary","text":"<p>.. _glossary:</p> <p>Glossary</p> <p>Nutanix Core ++++++++++++</p> <p>AOS ...</p> <p>AOS stands for Acropolis Operating System, and it is the OS running on the Controller VMs (CVMs).</p> <p>Pulse .....</p> <p>Pulse provides diagnostic system data to Nutanix customer support teams so that they can deliver proactive, context-aware support for Nutanix solutions.</p> <p>Prism Element .............</p> <p>Prism Element is the native management plane for Nutanix. Because its design is based on consumer product interfaces, it is more intuitive and easier to use than many enterprise application interfaces.</p> <p>Prism Central .............</p> <p>Prism Central is the multicloud control and management interface for Nutanix. Prism Central can manage multiple Nutanix clusters and serves as an aggregation point for monitoring and analytics.</p> <p>Node ....</p> <p>Industry standard x86 server with server-attached SSD and optional HDD (All Flash &amp; Hybrid Options).</p> <p>Block .....</p> <p>2U rack mount chassis that contains 1, 2 or 4 nodes with shared power and fans, and no shared no backplane.</p> <p>Storage Pool ............</p> <p>A storage pool is a group of physical storage devices including PCIe SSD, SSD, and HDD devices for the cluster.</p> <p>Storage Container .................</p> <p>A container is a subset of available storage used to implement storage policies.</p> <p>Anatomy of a Read I/O .....................</p> <p>Performance and Availability</p> <ul> <li>Data is read locally</li> <li>Remote access only if data is not locally present</li> </ul> <p>Anatomy of a Write I/O ......................</p> <p>Performance and Availability</p> <ul> <li>Data is written locally</li> <li>Replicated on other nodes for high availability</li> <li>Replicas are spread across cluster for high performance</li> </ul> <p>Nutanix Karbon +++++++++++++++</p> <p>Nutanix Karbon is an enterprise-grade Kubernetes Certified distribution that simplifies the provisioning, operations and lifecycle management of Kubernetes clusters with a native Kubernetes experience. Karbon makes it simple to deploy and maintain highly available Kubernetes cluster and operate web-scale workloads.</p>"},{"location":"appendix/helm/","title":"Helm","text":"<p>Helm is a package manager for Kubernetes based applications and resources deployment.</p> <p>Helm also provides an easy way of accomplishing the following:</p> <ul> <li>Install and uninstall kubernetes applications</li> <li>Versioning complex kubernetes applications</li> <li>Upgrade and rollback kubernetes applications</li> </ul> <p>You can think of Helm as <code>yum</code> or <code>apt-get</code> package managers in Linux.</p> <p>Helm has the following main components (not limited to):</p> <ul> <li>Helm command line tool - provides interface to all Helm functionality</li> <li>Charts - charts are manifests (yaml) files for desired kubernetes resources</li> <li>Value file(s) - all customisable values for the application deployment can be configured here</li> </ul> <p>Note<p>Helm used to be a server-client application. Helm being the client and Tiller server component being installed in a kubernetes cluster. With Helm 3.x onwards, Tiller is removed and only Helm is used for ease.</p> </p>"},{"location":"appendix/helm/#installing-helm-3x","title":"Installing Helm 3.x","text":"<p>Use the following commands to install helm in your LinuxTools VM.</p> <p>Install latest Helm through a script<pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> Verify Helm version<pre><code>helm version\n</code></pre></p>"},{"location":"appendix/linux_tools_vm/","title":"Linux Tools VM","text":""},{"location":"appendix/linux_tools_vm/#overview","title":"Overview","text":"<p>We will deploy a Linux (CentOS) VM to use it as our workstation. </p>"},{"location":"appendix/linux_tools_vm/#deploying-linux-tools-vm","title":"Deploying Linux Tools VM","text":"<ol> <li> <p>In Prism Central &gt; select Menu &gt; Compute and Storage &gt; VMs, and click Create VM</p> </li> <li> <p>Fill out the following fields:</p> <ul> <li>Name - Initials-Linux-ToolsVM</li> <li>Description - (Optional) Description for your VM.</li> <li>Number of VMs - 1</li> <li>CPU(s) - 4</li> <li>Number of Cores per CPU - 1</li> <li>Memory - 4 GiB</li> </ul> </li> <li>Click Next</li> <li>Under Disks select Attach Disk<ul> <li>Type - DISK</li> <li>Operation - Clone from Image</li> <li>Image - Linux_ToolsVM.qcow2</li> <li>Capacity - leave at default size</li> <li>Bus Type - leave at default SCSI Setting</li> </ul> </li> <li>Click Save</li> <li>Under Networks select Attach to Subnet<ul> <li>VLAN Name - Primary</li> <li>Network Connection State - Connected</li> <li>Assignment Type - Assign with DHCP</li> </ul> </li> <li>Click Save</li> <li>Click Next at the bottom</li> <li> <p>In Management section</p> <ul> <li>Categories - leave blank</li> <li>Timezone - leave at default UTC</li> <li> <p>Guest Customization - </p> <ul> <li>Script Type - Cloud-init (Linux)</li> <li>Configuration Method - Custom Script </li> </ul> <p>Do you need to create a SSH key pair?</p> <p>You can use any online ssh key generator if you are using Windows. Execute the following commands in you are in a Linux / Mac environment to generate a private key.</p> <p><pre><code>ssh-keygen -t rsa -b 2048 -C \"Created for Linux Tools VM\"\n# follow prompts \n# do not specify passphrase\n# once completed run the following command\ncat id_rsa.pub\n\n# copy the contents of the id_rsa.pub file to your cloudinit yaml file\n</code></pre> </p> <ul> <li>Paste the following script in the script window </li> </ul> <pre><code>#cloud-config\n# Set the hostname\nhostname: myhost\n# Create a new user\nusers:\n- default\n- name: nutanix\ngroups: wheel, users\nssh_authorized_keys:\n# Paste the generated public key here\n- ssh-rsa AAAAB3NzaC1....\n# You can also use the salter 1N or 6N format using OPENSSL (openssl passwd -1 -salt SaltSalt \"yourplaintextpassword\")\n# Paste the generated password here\npasswd: $1$SaltSalt$aOsqVFP2QULyFo5JYkOYB/\nshell: /bin/bash\nlock-passwd: false\nssh_pwauth: True                 # Enable password authentication for root\nssh_pwauth: True\n# Run additional commands\nruncmd:\n- 'sleep 10' # sleeping for the network to be UP\n- 'echo \"nutanix ALL=(ALL) NOPASSWD: ALL\" &gt;&gt; /etc/sudoers'\n# Run package upgrade\npackage_upgrade: true\n# Install the following packages - add extra that you would need\npackages:\n- git\n- bind-utils\n- nmap\n- curl\n- wget - vim\n- python3\n- python3-pip\n</code></pre> <li> <p>Click on Next</p> </li> <li>Click Create VM at the bottom</li> <li>Go back to Prism Central &gt; Menu &gt; Compute and Storage &gt; VMs</li> <li>Select your Initials-Linux-ToolsVM</li> <li> <p>Under Actions drop-down menu, choose Power On</p> <p>Note<p>It may take up to 10 minutes for the VM to be ready.</p> <p>You can watch the console of the VM from Prism Central to make sure all the clouinit script has finished running.</p> </p> </li> <li> <p>Login to the VM via SSH or Console session, using the following command:</p> <p><pre><code>ssh -i &lt;your_private_key&gt; -l nutanix &lt;IP of LinuxToolsVM&gt;\n</code></pre> Example command<pre><code>ssh -i id_rsa -l nutanix 10.54.63.95\n</code></pre></p> </li>"},{"location":"appendix/privatereg/","title":"Using Public Container Registry","text":"<p>In this section we will go through a few steps to use your own registry and associated credentials.</p> <p>Since most public container registries shape/rate-limit download, it may be essential to you use a private registry or use your login credentials to download container images from a public registry (Docker, etc).</p> <p>In kubernets, we can configure the service accounts to use your public registry credentials to download container images. The steps include the following:</p> <ol> <li>Create a generic kubernetes secret in your namespace with public registry credentials - this will become your pull secret</li> <li>Associate a kubernetes service account to use your pull secret</li> </ol>"},{"location":"appendix/privatereg/#create-a-container-pull-secret","title":"Create a Container Pull Secret","text":"<ol> <li>Login to your Linux Tools VM</li> <li>Authenticate to your kubernetes cluster using the kubeconfig file your downloaded from Karbon</li> <li>Change to the namespace you would like to use    <pre><code>kubectl config set-context --current --namespace=&lt;your-namespace&gt;\n</code></pre> Example command<pre><code>kubectl config set-context --current --namespace=default\n</code></pre></li> <li>Create a pull secret using your Docker account    <pre><code>kubectl create secret generic regcred \\\n--docker-username=DUMMY_USERNAME --docker-password=DUMMY_DOCKER_PASSWORD \\\n--docker-email=DUMMY_DOCKER_EMAIL\n</code></pre></li> </ol>"},{"location":"appendix/privatereg/#associate-pull-secret-with-service-account","title":"Associate Pull Secret with Service Account","text":"<ol> <li>Associate pull secret with the service account your will be using    <pre><code>kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"regcred\"}]}'\n</code></pre></li> <li> <p>Verify if the serviceaccount is using your <code>regcred</code> docker registy secret</p> <pre><code>kubectl get sa default -o yaml\n</code></pre> <p>Command output<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: default\nnamespace: default\nuid: 052fb0f4-3d50-11e5-b066-42010af0d7b6\nimagePullSecrets:\n- name: regcred\n</code></pre> 6. Now your yaml manifest can use this secret to download container images</p> <p>Create a pod<pre><code>k run nginx --image=nginx --restart=Never\n</code></pre> <pre><code>k get po nginx -oyaml\n</code></pre> Command output if the pod was created by default service account<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: null\nlabels:\nrun: nginx\nname: nginx\nspec:\ncontainers:\n- image: nginx\nname: nginx\ndnsPolicy: ClusterFirst\nrestartPolicy: Never\nimagePullSecrets:\n- name: regcred\n</code></pre> Delete the pod<pre><code>k delete po nginx\n</code></pre></p> </li> </ol> <p>You can now use your docker credentials to download container images from Docker public registry.</p>"}]}